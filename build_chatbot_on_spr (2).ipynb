{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook is used to demonstrate how to build a talking chatbot on 4th Generation of IntelÂ® XeonÂ® Scalable Processors Sapphire Rapids.\n",
    "\n",
    "The 4th Generation of IntelÂ® XeonÂ® Scalable processor provides two instruction sets viz. AMX_BF16 and AMX_INT8 which provides acceleration for bfloat16 and int8 operations respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install intel extension for transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements_cpu.txt\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your chatbot ðŸ’»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving NeuralChat the textual instruction, it will respond with the textual response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fb5724073248b3bd69f6973156ac11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Intel Xeon Scalable Processors represent a family of high-performance central processing units (CPUs) designed for data centers, cloud computing, and other demanding workloads. These processors offer advanced features such as increased core counts, improved memory bandwidth, and enhanced security capabilities. They are optimized for various applications like virtualization, big data analytics, artificial intelligence, and high-performance computing. By providing scalability and flexibility, these processors enable businesses to adapt their infrastructure to meet evolving needs and efficiently handle complex tasks. Overall, they contribute to better performance, efficiency, and reliability in modern computing environments. Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½Ñ‚ÐµÐ» Ð¸Ð½\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc5212384d1463b997acecb54267576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To utilize IntelÂ® oneAPI for developing AI applications, follow these steps:\n",
      "\n",
      "1. Familiarize yourself with the IntelÂ® oneAPI toolkit: It's a unified programming model designed to simplify cross-architecture development across various domains like HPC, data analytics, machine learning, and graphics.\n",
      "\n",
      "2. Choose your preferred programming language: The IntelÂ® oneAPI supports multiple languages such as C++, Fortran, Python, and OpenCL. Select the language that best suits your project requirements.\n",
      "\n",
      "3. Install the required software: Download and install the appropriate version of the IntelÂ® oneAPI toolkit from their official website. Make sure you have the necessary dependencies installed on your system.\n",
      "\n",
      "4. Learn about the available libraries and frameworks: Explore the different libraries and frameworks within the IntelÂ® oneAPI toolkit, including the Data Analytics Library (DAAL), Deep Neural Network Library (DNNCNN), and the High Performance Computing Toolkit (HPC). These libraries provide optimized algorithms and tools for specific tasks in AI development.\n",
      "\n",
      "5. Start building your AI application: Begin by creating a basic structure for your AI application using the chosen programming language.\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "321838d5df024107ba1a7b7123a40bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IntelÂ® Neural Chat offers numerous advantages when it comes to creating intelligent chatbots. Some key benefits include:\n",
      "\n",
      "1. Natural Language Processing (NLP): It utilizes advanced NLP algorithms to understand human language better, enabling your chatbot to engage in meaningful conversations with users.\n",
      "\n",
      "2. Contextual Understanding: The ability to comprehend context allows the chatbot to provide relevant responses based on previous interactions or topics discussed. This enhances the overall user experience.\n",
      "\n",
      "3. Personalization: With its machine learning capabilities, IntelÂ® Neural Chat can adapt to individual users' preferences and needs, making each interaction feel personalized.\n",
      "\n",
      "4. Scalability: As the chatbot learns from user interactions, it becomes more efficient at handling various queries and tasks. This scalability ensures that the chatbot can grow alongside your business requirements.\n",
      "\n",
      "5. Security: IntelÂ® Neural Chat is designed with security in mind, ensuring that sensitive data remains protected during the conversation process.\n",
      "\n",
      "6. Cost-effective: By automating customer support and providing instant assistance, chatbots powered by IntelÂ® Neural Chat can reduce operational costs while improving efficiency.\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa8e28aad474513bc946545935e0dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide you with the most recent updates, I'll need to check for new announcements. Please give me a moment to gather the latest news.\n",
      "\n",
      "Google Pixel 6a Announced: The mid-range smartphone features a Tensor chipset, dual rear cameras, and a 6.1-inch FHD+ display. It is set to release in July 2022.\n",
      "\n",
      "Google Pixel Watch Unveiled: This smartwatch comes with Wear OS powered by Samsung's Exynos W92 processor, Fitbit integration, and various health tracking features. It is expected to launch alongside the Pixel 7 series later this year.\n",
      "\n",
      "Google Pixel Buds Pro Launched: These premium wireless earbuds offer active noise cancellation, spatial audio support, and up to 11 hours of listening time on a single charge. They are available now.\n",
      "\n",
      "Google Pixel Tablet Teased: Although not officially announced yet, Google has hinted at a tablet device running Android 13 and featuring a detachable keyboard. More details are expected soon.\n",
      "\n",
      "These are some of the recent product announcements from Google.\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa9ad70f58461ba9036d4b3cdaa0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man walks into a library looking for a book about jokes. As he browses through the shelves, he notices two books side by side - one titled \"100 Funniest Jokes\" and another called \"The Worst Jokes Ever.\" He picks up both books and starts reading them simultaneously. After a while, he realizes that he's laughing at the same jokes in both books. Confused, he asks a librarian, \"Why are these two books filled with identical jokes?\" The librarian replies, \"Well, it's because they're actually from the same book. They just split it into two parts to double their sales.\"ä¸¶ä¸¶ä¸¶naioä¸¶naionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaionaio\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c89743021349a8aa47d04988ee7403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if your Intel processor is compatible with the latest version of TensorFlow, please check the system requirements for TensorFlow on their official website (https://www.tensorflow.org/). This will provide you with specific details about the CPU architecture and other necessary components needed for running TensorFlow efficiently. Ñ–Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚Ñ€ Ð¸Ð½Ð´ÐµÑÑ‚\n",
      "Loading model Intel/neural-chat-7b-v3-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5c714011ca46e080fc83b2246736c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The limitations of using Neural Chat for building complex chatbots can include:\n",
      "\n",
      "1. Limited context understanding: While neural networks can learn from vast amounts of data, they may struggle with comprehending complex or nuanced conversations. This might result in misunderstandings or inappropriate responses.\n",
      "\n",
      "2. Data bias: If the training data used to create the neural network has biases, these biases could be reflected in the chatbot's behavior. For example, if the training data is predominantly male-centric, the chatbot might not respond well to female users.\n",
      "\n",
      "3. Lack of common sense: Neural networks excel at pattern recognition but may have difficulty applying logic or reasoning beyond what they have learned from their training data. This can lead to unrealistic or illogical responses when faced with new situations.\n",
      "\n",
      "4. Security concerns: As AI systems become more advanced, there is always a risk of malicious actors exploiting vulnerabilities in the system. In the case of chatbots, this could potentially lead to security breaches or misuse of personal information.\n",
      "\n",
      "5. High computational costs: Training large neural networks requires significant computing resources, which can be expensive and time-consum\n"
     ]
    }
   ],
   "source": [
    "# BF16 Optimization\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"How can I use IntelÂ® oneAPI to develop AI applications?\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"What are the benefits of using IntelÂ® Neural Chat for building chatbots?\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"What are the latest product announcements from Google?\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me a joke!\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Is my Intel processor compatible with the latest version of TensorFlow?\")\n",
    "print(response)\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"What are the limitations of using Neural Chat for building complex chatbots?\")\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Chat With Retrieval Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User could also leverage NeuralChat Retrieval plugin to do domain specific chat by feding with some documents like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docs\n",
    "%cd docs\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/docs/sample.jsonl\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/docs/sample.txt\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/docs/sample.xlsx\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args[\"input_path\"]=\"./docs/\"\n",
    "config = PipelineConfig(plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(\"How many cores does the IntelÂ® XeonÂ® Platinum 8480+ Processor have in total?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voice Chat with ASR & TTS Plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of voice chat, users have the option to engage in various modes: utilizing input audio and receiving output audio, employing input audio and receiving textual output, or providing input in textual form and receiving audio output.\n",
    "\n",
    "For the Python API code, users have the option to enable different voice chat modes by setting ASR and TTS plugins enable or disable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/pipeline/plugins/audio/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/speaker_embeddings/spk_embed_default.pt\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/audio/sample.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.tts.enable = True\n",
    "plugins.tts.args[\"output_audio_path\"] = \"./response.wav\"\n",
    "plugins.asr.enable = True\n",
    "\n",
    "config = PipelineConfig(plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "result = chatbot.predict(query=\"./sample.wav\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low Precision Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BF16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BF16 Optimization\n",
    "from intel_extension_for_transformers.neural_chat.config import PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hustler",
   "language": "python",
   "name": "hustler"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
